{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Taking Care of Dependencies!","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torch","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchtext==0.6.0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nfrom transformers import BertTokenizer, BertModel\n\nimport numpy as np\n\nimport time\nimport random\nimport functools","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checkout some essential tokens","metadata":{}},{"cell_type":"code","source":"init_token = tokenizer.cls_token\npad_token = tokenizer.pad_token\nunk_token = tokenizer.unk_token\n\nprint(init_token, pad_token, unk_token)\n\ninit_token_idx = tokenizer.convert_tokens_to_ids(init_token)\npad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\nunk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n\nprint(init_token_idx, pad_token_idx, unk_token_idx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading BertConfig","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig\n\nconfig = BertConfig.from_pretrained('bert-base-uncased')\nmax_input_length = config.max_position_embeddings\n\nprint(max_input_length)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cut_and_convert_to_id(tokens, tokenizer, max_input_length):\n    tokens = tokens[:max_input_length-1]\n    tokens = tokenizer.convert_tokens_to_ids(tokens)\n    return tokens\n\ndef cut_to_max_length(tokens, max_input_length):\n    tokens = tokens[:max_input_length-1]\n    return tokens","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_preprocessor = functools.partial(cut_and_convert_to_id,\n                                      tokenizer = tokenizer,\n                                      max_input_length = max_input_length)\n\ntag_preprocessor = functools.partial(cut_to_max_length,\n                                     max_input_length = max_input_length)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making use of the inbuilt dataset","metadata":{}},{"cell_type":"code","source":"from torchtext.data  import Field\n\nTEXT = data.Field(use_vocab = False,\n                  lower = True,\n                  preprocessing = text_preprocessor,\n                  init_token = init_token_idx,\n                  pad_token = pad_token_idx,\n                  unk_token = unk_token_idx)\n\nUD_TAGS = data.Field(unk_token = None,\n                     init_token = '<pad>',\n                     preprocessing = tag_preprocessor)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, valid_data, test_data = datasets.UDPOS.splits(fields)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's see the first data point","metadata":{}},{"cell_type":"code","source":"print(vars(train_data.examples[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See all thw 18 tags here","metadata":{}},{"cell_type":"code","source":"UD_TAGS.build_vocab(train_data)\n\nprint(UD_TAGS.vocab.stoi)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size = BATCH_SIZE,\n    device = device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"code","source":"class BERTPoSTagger(nn.Module):\n    def __init__(self,\n                 bert,\n                 output_dim,\n                 dropout):\n\n        super().__init__()\n\n        self.bert = bert\n\n        embedding_dim = bert.config.to_dict()['hidden_size']\n\n        self.fc = nn.Linear(embedding_dim, output_dim)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n\n        #text = [sent len, batch size]\n\n        text = text.permute(1, 0)\n\n        #text = [batch size, sent len]\n\n        embedded = self.dropout(self.bert(text)[0])\n\n        #embedded = [batch size, seq len, emb dim]\n\n        embedded = embedded.permute(1, 0, 2)\n\n        #embedded = [sent len, batch size, emb dim]\n\n        predictions = self.fc(self.dropout(embedded))\n\n        #predictions = [sent len, batch size, output dim]\n\n        return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the Bert Model","metadata":{}},{"cell_type":"code","source":"bert = BertModel.from_pretrained('bert-base-uncased')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Starting with Training!","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIM = len(UD_TAGS.vocab)\nDROPOUT = 0.25\n\nmodel = BERTPoSTagger(bert,\n                      OUTPUT_DIM,\n                      DROPOUT)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 5e-5\n\noptimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n\ncriterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\ncriterion = criterion.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def categorical_accuracy(preds, y, tag_pad_idx):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n    non_pad_elements = (y != tag_pad_idx).nonzero()\n    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Write your code in here for train() and evaluate() functions.","metadata":{}},{"cell_type":"markdown","source":"# After that run for different epochs as given in the list values","metadata":{}},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, tag_pad_idx):\n  #Write your code here\n\n\n  pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, tag_pad_idx):\n  #Write your code here\n\n\n  pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Try running from different epochs from the space\n# epoch = [2,5,8,10,12,15]\n\nN_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n\n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, TAG_PAD_IDX)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, TAG_PAD_IDX)\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'tut2-model.pt')\n\n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(torch.load('tut2-model.pt'))\n\ntest_loss, test_acc = evaluate(model, test_iterator, criterion, TAG_PAD_IDX)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tag_sentence(model, device, sentence, tokenizer, text_field, tag_field):\n\n    model.eval()\n\n    if isinstance(sentence, str):\n        tokens = tokenizer.tokenize(sentence)\n    else:\n        tokens = sentence\n\n    numericalized_tokens = tokenizer.convert_tokens_to_ids(tokens)\n    numericalized_tokens = [text_field.init_token] + numericalized_tokens\n\n    unk_idx = text_field.unk_token\n\n    unks = [t for t, n in zip(tokens, numericalized_tokens) if n == unk_idx]\n\n    token_tensor = torch.LongTensor(numericalized_tokens)\n\n    token_tensor = token_tensor.unsqueeze(-1).to(device)\n\n    predictions = model(token_tensor)\n\n    top_predictions = predictions.argmax(-1)\n\n    predicted_tags = [tag_field.vocab.itos[t.item()] for t in top_predictions]\n\n    predicted_tags = predicted_tags[1:]\n\n    assert len(tokens) == len(predicted_tags)\n\n    return tokens, predicted_tags, unks","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentence = 'A quick brown fox jumps over the lazy dog'\n\ntokens, tags, unks = tag_sentence(model,\n                                  device,\n                                  sentence,\n                                  tokenizer,\n                                  TEXT,\n                                  UD_TAGS)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The correct output you should obtain is [a_DET quick_ADJ brown_ADJ fox_NOUN jumped_VERB over_ADP the_DET lazy_ADJ dog_NOUN","metadata":{}},{"cell_type":"code","source":"print(\"Pred. Token\\tTag\\n\")\n\nfor token, tag in zip(tokens, tags):\n    print(token, '_', token)","metadata":{},"execution_count":null,"outputs":[]}]}